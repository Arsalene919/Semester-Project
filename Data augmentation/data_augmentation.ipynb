{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02ca938f696f42398b5b9c2f19081825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bf97fe6b11e41a2b9332d52fe0c9b58",
              "IPY_MODEL_ed315006ddce4adc833ea57a707082a4",
              "IPY_MODEL_16ee22c8a9ba4047b114497d7c861b8d"
            ],
            "layout": "IPY_MODEL_ff85e76519e142638dc1dd19cb28e5b1"
          }
        },
        "5bf97fe6b11e41a2b9332d52fe0c9b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_277c2f22ca9b4a4bb64328a868fecce3",
            "placeholder": "​",
            "style": "IPY_MODEL_e695cfe11de64650aee3766c4cdebd78",
            "value": "Fetching 13 files: 100%"
          }
        },
        "ed315006ddce4adc833ea57a707082a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3847d34ac8054e5c897d3f4e29f16d13",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26f0f764851d4d5d9c128e968b5e3e3c",
            "value": 13
          }
        },
        "16ee22c8a9ba4047b114497d7c861b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26a5291de1f547fba4749ac4cd4464ed",
            "placeholder": "​",
            "style": "IPY_MODEL_8fff1348ce764cf38cf0a3a5922f795e",
            "value": " 13/13 [00:00&lt;00:00, 587.67it/s]"
          }
        },
        "ff85e76519e142638dc1dd19cb28e5b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "277c2f22ca9b4a4bb64328a868fecce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e695cfe11de64650aee3766c4cdebd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3847d34ac8054e5c897d3f4e29f16d13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f0f764851d4d5d9c128e968b5e3e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26a5291de1f547fba4749ac4cd4464ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fff1348ce764cf38cf0a3a5922f795e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13bb6279cce241e2b6b0b883a18883fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_133f43bbdd1c4248804c77f4d789d596",
              "IPY_MODEL_0de335f20db6487689fdd8a0e40cd925",
              "IPY_MODEL_69aebe082bc94074ad4ff051dd693c70"
            ],
            "layout": "IPY_MODEL_95953fbc27f04902b8d5cc88c8efe39b"
          }
        },
        "133f43bbdd1c4248804c77f4d789d596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752b3489937a47ddbd8050cd8c861701",
            "placeholder": "​",
            "style": "IPY_MODEL_20bad400ea4b46189980ce3c367ca124",
            "value": "Fetching 13 files: 100%"
          }
        },
        "0de335f20db6487689fdd8a0e40cd925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7219e63899384c01874ac2f9e6b6682c",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5c37b863f1b4c0d990e9825cd2e41a6",
            "value": 13
          }
        },
        "69aebe082bc94074ad4ff051dd693c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77071a0325d45e38e4127bb656fe9c0",
            "placeholder": "​",
            "style": "IPY_MODEL_cde58ad3d67c4f38bd0fbc898291267d",
            "value": " 13/13 [00:00&lt;00:00, 791.68it/s]"
          }
        },
        "95953fbc27f04902b8d5cc88c8efe39b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "752b3489937a47ddbd8050cd8c861701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20bad400ea4b46189980ce3c367ca124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7219e63899384c01874ac2f9e6b6682c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c37b863f1b4c0d990e9825cd2e41a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c77071a0325d45e38e4127bb656fe9c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cde58ad3d67c4f38bd0fbc898291267d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9834mXbYcau",
        "outputId": "c689aa20-bb05-4156-d0c7-42c89e9c2f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install autoawq"
      ],
      "metadata": {
        "id": "-frTbvrlVk-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c3d8ec-5b31-4525-8625-751398feea4b",
        "execution": {
          "iopub.status.busy": "2024-05-04T11:32:15.704701Z",
          "iopub.execute_input": "2024-05-04T11:32:15.704987Z",
          "iopub.status.idle": "2024-05-04T11:32:28.578661Z",
          "shell.execute_reply.started": "2024-05-04T11:32:15.704960Z",
          "shell.execute_reply": "2024-05-04T11:32:28.577445Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autoawq\n",
            "  Downloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from autoawq) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.35.0 in /usr/local/lib/python3.10/dist-packages (from autoawq) (4.40.1)\n",
            "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from autoawq) (4.11.0)\n",
            "Collecting accelerate (from autoawq)\n",
            "  Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from autoawq)\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from autoawq)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autoawq-kernels (from autoawq)\n",
            "  Downloading autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl (33.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.4/33.4 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.12.1->autoawq) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.1->autoawq)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->autoawq) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->autoawq) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->autoawq) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->autoawq)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->autoawq) (2.0.3)\n",
            "Collecting xxhash (from datasets->autoawq)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->autoawq)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->autoawq) (3.9.5)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.12.1->autoawq)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->autoawq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->autoawq) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->autoawq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->autoawq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->autoawq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->autoawq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.35.0->autoawq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.35.0->autoawq) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.35.0->autoawq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.35.0->autoawq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->autoawq) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->autoawq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->autoawq) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->autoawq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->autoawq) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->autoawq) (1.16.0)\n",
            "Installing collected packages: zstandard, xxhash, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, nvidia-cusolver-cu12, datasets, autoawq-kernels, accelerate, autoawq\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed accelerate-0.30.0 autoawq-0.2.5 autoawq-kernels-0.0.6 datasets-2.19.0 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 xxhash-3.4.1 zstandard-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/casper-hansen/AutoAWQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstZL_oBV4O9",
        "outputId": "3792e82d-700d-4976-9220-f0f6b18a02bd",
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AutoAWQ'...\n",
            "remote: Enumerating objects: 3065, done.\u001b[K\n",
            "remote: Counting objects: 100% (1600/1600), done.\u001b[K\n",
            "remote: Compressing objects: 100% (750/750), done.\u001b[K\n",
            "remote: Total 3065 (delta 1152), reused 1048 (delta 830), pack-reused 1465\u001b[K\n",
            "Receiving objects: 100% (3065/3065), 7.52 MiB | 21.20 MiB/s, done.\n",
            "Resolving deltas: 100% (1899/1899), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd AutoAWQ"
      ],
      "metadata": {
        "id": "xO6lL3oFV9aD",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract causal text from the generated examples\n",
        "def extract_causal_text(lines):\n",
        "    causal_text = []\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        if \"LLM output:\" in lines[i]:\n",
        "            causal_text.append(lines[i+1])\n",
        "            i += 2\n",
        "        else:\n",
        "            i += 1\n",
        "    return causal_text"
      ],
      "metadata": {
        "trusted": true,
        "id": "8NRzXGelYani"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_s2(sentence, cause, effect, signal):\n",
        "    \"\"\"\n",
        "    Create an example based on the tags used in the appropriate scheme.\n",
        "\n",
        "    Parameters:\n",
        "    - sentence (str): The original sentence.\n",
        "    - cause (str): The cause enclosed between tags.\n",
        "    - effect (str): The effect enclosed between tags.\n",
        "    - signal (str): The signal enclosed between tags.\n",
        "\n",
        "    Returns:\n",
        "    - str: Example with tags replaced by their values.\n",
        "    \"\"\"\n",
        "    example = f\"Here is an example of the tags used in the appropriate scheme:\\n{sentence}\\n\" \\\n",
        "              f\"Where the cause in the scheme is: {cause}\\n\" \\\n",
        "              f\"The effect in the scheme is: {effect}\\n\" \\\n",
        "              f\"The signal in the scheme is: {signal}\\n\"\n",
        "    return example"
      ],
      "metadata": {
        "id": "AGD59drz-LkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-AWQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
        "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
        "                                          trust_remote_code=False, safetensors=True)\n",
        "\n",
        "# Define domains\n",
        "domains = ['economic events', 'social events', 'political unrest', 'elections', 'natural disasters',\n",
        "           'transport accidents', 'crimes', 'industrial accidents', 'international relations',\n",
        "           'home policy', 'MUC-like events', 'KBP-like events']\n",
        "\n",
        "# Request template\n",
        "request = \"generate me 5 causal examples\"\n",
        "\n",
        "# Definition of tags\n",
        "definition = \"\"\"Below are the definitions for the tags in the sentence:\n",
        "  Cause: The reason for an event happening, to be enclosed between <ARG0> and </ARG0>.\n",
        "  Effect: The event that occurs due to the cause, to be enclosed between <ARG1> and </ARG1>.\n",
        "  Signal: Words that transition the cause to the effect, to be enclosed between <SIG0> and </SIG0>.\n",
        "\n",
        "  Please generate causal sentences within this domain.\"\"\"\n",
        "\n",
        "# Example based on an entry of the original training dataset\n",
        "example_sentence = \"A decision by the United Nations to impose economic sanctions on a country led to a decrease in the country's economic activity and standard of living\"\n",
        "example_cause = \"<ARG0> A decision by the United Nations to impose economic sanctions on a country.\"\n",
        "example_signal = \"<SIG0> Led to.\"\n",
        "example_effect = \"<ARG1> A decrease in the country's economic activity and standard of living.\"\n",
        "\n",
        "# Saving generated examples to a text file\n",
        "file_path = \"generated_text.txt\"\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    for d in domains:\n",
        "        # Prompt template with revised structure\n",
        "        prompt_template = f'''\n",
        "        </s>\n",
        "        {request} in the domain of {d}\n",
        "\n",
        "        {definition}\n",
        "\n",
        "        {conv_s2(example_sentence, example_cause, example_signal, example_effect)}\n",
        "        '''\n",
        "\n",
        "        # Tokenize prompt\n",
        "        token_input = tokenizer(\n",
        "            prompt_template,\n",
        "            return_tensors='pt'\n",
        "        ).input_ids.cuda()\n",
        "\n",
        "        # Generate causal examples\n",
        "        generation_output = model.generate(\n",
        "            token_input,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            top_k=40,\n",
        "            max_new_tokens=1024\n",
        "        )\n",
        "\n",
        "        # Decode and structure generated text\n",
        "        token_output = generation_output[0]\n",
        "        text_output = tokenizer.decode(token_output)\n",
        "        # Remove the prompt\n",
        "        text_output = text_output.replace(request, \"\").strip()\n",
        "        text_output = text_output.replace(definition, \"\").strip()\n",
        "        text_output = text_output.replace(prompt_template, \"\").strip()\n",
        "        structured_text_output = text_output.replace(\"<ARG0>\", \"<ARG0>\").replace(\"</ARG0>\", \"</ARG0>\") \\\n",
        "            .replace(\"<ARG1>\", \"<ARG1>\").replace(\"</ARG1>\", \"</ARG1>\") \\\n",
        "            .replace(\"<SIG0>\", \"<SIG0>\").replace(\"</SIG0>\", \"</SIG0>\").strip()\n",
        "        # Write to file\n",
        "        file.write(f\"{structured_text_output}\\n\\n\")"
      ],
      "metadata": {
        "id": "IOgxYbXfWl9D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "02ca938f696f42398b5b9c2f19081825",
            "5bf97fe6b11e41a2b9332d52fe0c9b58",
            "ed315006ddce4adc833ea57a707082a4",
            "16ee22c8a9ba4047b114497d7c861b8d",
            "ff85e76519e142638dc1dd19cb28e5b1",
            "277c2f22ca9b4a4bb64328a868fecce3",
            "e695cfe11de64650aee3766c4cdebd78",
            "3847d34ac8054e5c897d3f4e29f16d13",
            "26f0f764851d4d5d9c128e968b5e3e3c",
            "26a5291de1f547fba4749ac4cd4464ed",
            "8fff1348ce764cf38cf0a3a5922f795e"
          ]
        },
        "outputId": "a8858bb1-46b8-4900-ee34-f51c206ea55a",
        "execution": {
          "iopub.status.busy": "2024-05-04T11:32:34.742558Z",
          "iopub.execute_input": "2024-05-04T11:32:34.742904Z",
          "iopub.status.idle": "2024-05-04T11:32:57.491897Z",
          "shell.execute_reply.started": "2024-05-04T11:32:34.742872Z",
          "shell.execute_reply": "2024-05-04T11:32:57.490305Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02ca938f696f42398b5b9c2f19081825"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Replacing layers...: 100%|██████████| 32/32 [00:14<00:00,  2.26it/s]\n",
            "Fusing layers...: 100%|██████████| 32/32 [00:00<00:00, 90.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-AWQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
        "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
        "                                          trust_remote_code=False, safetensors=True)\n",
        "\n",
        "# Define domains\n",
        "domains = ['economic events', 'social events', 'political unrest', 'elections', 'natural disasters',\n",
        "           'transport accidents', 'crimes', 'industrial accidents', 'international relations',\n",
        "           'home policy', 'MUC-like events', 'KBP-like events']\n",
        "\n",
        "# Define request and definition\n",
        "request = \"generate me 5 causal examples\"\n",
        "definition = \"\"\"Below are the definitions for the tags in the sentence:\n",
        "  Cause: The reason for an event happening, to be enclosed between <ARG0> and </ARG0>.\n",
        "  Effect: The event that occurs due to the cause, to be enclosed between <ARG1> and </ARG1>.\n",
        "  Signal: Words that transition the cause to the effect, to be enclosed between <SIG0> and </SIG0>.\n",
        "\n",
        "  Please generate causal sentences within this domains.\"\"\"\n",
        "\n",
        "# Saving generated examples to a text file\n",
        "file_path = \"generated_examples.txt\"\n",
        "# Define the target number of outputs\n",
        "target_outputs = 500\n",
        "generated_count = 0\n",
        "with open(file_path, 'w') as file:\n",
        "    for d in domains:\n",
        "        prompt_template = f'''</s>\n",
        "\n",
        "        {request} in the domain of {d} {definition}\n",
        "\n",
        "        '''\n",
        "\n",
        "        # Tokenize prompt\n",
        "        token_input = tokenizer(\n",
        "            prompt_template,\n",
        "            return_tensors='pt'\n",
        "        ).input_ids.cuda()\n",
        "\n",
        "        # Generate causal examples\n",
        "        generation_output = model.generate(\n",
        "            token_input,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            top_k=40,\n",
        "            max_new_tokens=512\n",
        "        )\n",
        "\n",
        "        # Decode and structure generated text\n",
        "        token_output = generation_output[0]\n",
        "        text_output = tokenizer.decode(token_output)\n",
        "        structured_text_output = text_output.replace(\"<ARG0>\", \"<ARG0>\").replace(\"</ARG0>\", \"</ARG0>\") \\\n",
        "            .replace(\"<ARG1>\", \"<ARG1>\").replace(\"</ARG1>\", \"</ARG1>\") \\\n",
        "            .replace(\"<SIG0>\", \"<SIG0>\").replace(\"</SIG0>\", \"</SIG0>\")\n",
        "        # Write to file\n",
        "        file.write(d + \"\\n\")\n",
        "        file.write(structured_text_output + \"\\n\")"
      ],
      "metadata": {
        "id": "IsfbRoRZDL9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "13bb6279cce241e2b6b0b883a18883fa",
            "133f43bbdd1c4248804c77f4d789d596",
            "0de335f20db6487689fdd8a0e40cd925",
            "69aebe082bc94074ad4ff051dd693c70",
            "95953fbc27f04902b8d5cc88c8efe39b",
            "752b3489937a47ddbd8050cd8c861701",
            "20bad400ea4b46189980ce3c367ca124",
            "7219e63899384c01874ac2f9e6b6682c",
            "a5c37b863f1b4c0d990e9825cd2e41a6",
            "c77071a0325d45e38e4127bb656fe9c0",
            "cde58ad3d67c4f38bd0fbc898291267d"
          ]
        },
        "outputId": "5a336e3c-3263-4821-f066-c2506d8d927f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13bb6279cce241e2b6b0b883a18883fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Replacing layers...: 100%|██████████| 32/32 [00:09<00:00,  3.40it/s]\n",
            "Fusing layers...: 100%|██████████| 32/32 [00:00<00:00, 90.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-AWQ\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
        "# Load model\n",
        "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
        "                                          trust_remote_code=False, safetensors=True)\n",
        "\n",
        "domains = ['economic events', 'social events', 'political unrest', 'elections', 'natural disasters', 'transport accidents', 'crimes', 'industrial accidents', 'international relations', 'home policy', 'MUC-like events', 'KBP-like events']\n",
        "\n",
        "request = \"\"\"\n",
        "generate me 5 causal examples\n",
        "\"\"\"\n",
        "\n",
        "definition = \"\"\"Below are the definitions for the tags in the sentence:\n",
        "  Cause: The reason for an event happening, to be enclosed between <ARG0> and </ARG0>.\n",
        "  Effect: The event that occurs due to the cause, to be enclosed between <ARG1> and </ARG1>.\n",
        "  Signal: Words that transition the cause to the effect, to be enclosed between <SIG0> and </SIG0>.\n",
        "\n",
        "  Please generate causal sentences within this domains.\"\"\"\n",
        "\n",
        "# Define the target number of outputs\n",
        "target_outputs = 1000\n",
        "generated_count = 0\n",
        "\n",
        "# Open a file named 'domain.txt' in write mode\n",
        "with open('domain.txt', 'w') as file:\n",
        "    while generated_count < target_outputs:\n",
        "        for d in domains:\n",
        "            prompt_template = f'''\n",
        "            </s>\n",
        "\n",
        "            {request + 'in the domain of ' + str(d) + definition }</s>\n",
        "\n",
        "            '''\n",
        "            # Write the content of the string to the file\n",
        "            file.write(prompt_template)\n",
        "\n",
        "            # Generate output\n",
        "            token_input = tokenizer(\n",
        "                prompt_template,\n",
        "                return_tensors='pt'\n",
        "            ).input_ids.cuda()\n",
        "\n",
        "            # Generate output\n",
        "            generation_output = model.generate(\n",
        "                token_input,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                top_k=40,\n",
        "                max_new_tokens=512\n",
        "            )\n",
        "\n",
        "            # Get the tokens from the output, decode them, print them\n",
        "            token_output = generation_output[0]\n",
        "            text_output = tokenizer.decode(token_output)\n",
        "            print(\"LLM output: \", text_output)\n",
        "\n",
        "            generated_count += 1\n",
        "\n",
        "            if generated_count >= target_outputs:\n",
        "                break"
      ],
      "metadata": {
        "id": "-vkaNatUMsbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the txt file, concatenat the examples in a csv fil.\n"
      ],
      "metadata": {
        "id": "KM1fXCETDRPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def append_to_csv_file(filename, data):\n",
        "    \"\"\"\n",
        "    Append the given data to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - filename (str): The name of the CSV file.\n",
        "    - data (list): The data to be appended to the file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use 'a' mode to open the file in append mode\n",
        "        with open(filename, 'a', newline='') as csvfile:\n",
        "            # Create a CSV writer object\n",
        "            csv_writer = csv.writer(csvfile)\n",
        "\n",
        "            # Append each line from the data list to the CSV file\n",
        "            for line in data:\n",
        "                csv_writer.writerow([line.strip()])  # Remove newline characters before writing\n",
        "\n",
        "        print(f'Data appended to {filename} successfully.')\n",
        "    except Exception as e:\n",
        "        print(f'Error: {e}')\n",
        "\n",
        "# Path to the generated examples text file\n",
        "text_file_path = \"/content/generated_examples.txt\"\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_file_path = \"/content/generated_examples.csv\"\n",
        "\n",
        "# Read the content of the text file\n",
        "with open(text_file_path, 'r') as file:\n",
        "    text_content = file.readlines()\n",
        "\n",
        "# Append the content to the CSV file\n",
        "append_to_csv_file(csv_file_path, text_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ04CX3TzXms",
        "outputId": "33839984-70d2-4ae0-ecc0-ab6d56ad2e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data appended to /content/generated_examples.csv successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/generated_text.txt\"\n",
        "# Load the generated text from the file\n",
        "with open(file_path, 'r') as file:\n",
        "    generated_text = file.readlines()\n",
        "\n",
        "# Initialize lists to store data\n",
        "corpus_list = []\n",
        "doc_id_list = []\n",
        "sent_id_list = []\n",
        "eg_id_list = []\n",
        "index_list = []\n",
        "text_list = []\n",
        "causal_text_list = []\n",
        "num_rs_list = []\n",
        "\n",
        "# Populate the lists with data\n",
        "for index, text in enumerate(generated_text):\n",
        "    corpus_list.append(\"Generated\")\n",
        "    doc_id_list.append(\"1\")  # Assuming all belong to the same document\n",
        "    sent_id_list.append(index + 1)  # Assuming each line corresponds to a sentence\n",
        "    eg_id_list.append(\"1\")  # Assuming all examples belong to the same example group\n",
        "    index_list.append(index)\n",
        "    text_list.append(text)\n",
        "    causal_text_list.append(text)  # Assuming the causal text is the same as the generated text\n",
        "    num_rs_list.append(0)  # Assuming there are no root causes for now\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'corpus': corpus_list,\n",
        "    'doc_id': doc_id_list,\n",
        "    'sent_id': sent_id_list,\n",
        "    'eg_id': eg_id_list,\n",
        "    'index': index_list,\n",
        "    'text': text_list,\n",
        "    'causal_text': causal_text_list,\n",
        "    'num_rs': num_rs_list\n",
        "})\n",
        "\n",
        "# Splitting the text into sentences and removing the <s>, </s>, <ARG0>, <ARG1>, and <SIG0> tags\n",
        "sentences = []\n",
        "for text in df:\n",
        "    for sentence in text.split('\\n'):\n",
        "        sentence = sentence.strip()\n",
        "        if sentence:\n",
        "            sentence = sentence.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<ARG0>\", \"\").replace(\"</ARG0>\", \"\").replace(\"<ARG1>\", \"\").replace(\"</ARG1>\", \"\").replace(\"<SIG0>\", \"\").replace(\"</SIG0>\", \"\")\n",
        "            sentences.append(sentence)\n",
        "\n",
        "# Appending each sentence to the DataFrame\n",
        "for sentence in sentences:\n",
        "    # Assuming 'data' contains the necessary data to be appended for each sentence\n",
        "    data = ['Generated', '1', len(df) + 1, '1', len(df), sentence, sentence, 0]\n",
        "\n",
        "    # Append the data to the DataFrame\n",
        "    df.loc[len(df)] = data\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('generated_data_from_df.csv', index=False)"
      ],
      "metadata": {
        "id": "KcZayeBBA3Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Function to extract causal relations from a sentence\n",
        "def extract_causal_relations(sentence):\n",
        "    # Placeholder function, you would implement this according to your specific causal relation extraction method\n",
        "    # For demonstration purposes, let's assume no causal relations are extracted\n",
        "    return []\n",
        "\n",
        "# Function to write data to CSV file\n",
        "def write_to_csv(data, output_filename):\n",
        "    with open(output_filename, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['corpus', 'doc_id', 'sent_id', 'eg_id', 'index', 'text', 'causal_text', 'num_rs']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "# Load causal text file\n",
        "with open('/content/domain.txt', 'r') as file:\n",
        "    causal_text = file.read()\n",
        "\n",
        "# Tokenize into sentences\n",
        "sentences = sent_tokenize(causal_text)\n",
        "\n",
        "# Initialize data list to hold rows for CSV\n",
        "csv_data = []\n",
        "\n",
        "# Process each sentence\n",
        "for sent_id, sentence in enumerate(sentences, start=1):\n",
        "    causal_relations = extract_causal_relations(sentence)\n",
        "    eg_id = 0\n",
        "    for index, causal_text_w_pairs in enumerate(causal_relations, start=1):\n",
        "        eg_id += 1\n",
        "        num_rs = len(causal_text_w_pairs)\n",
        "        row = {\n",
        "            'corpus': 'Your Corpus Name',\n",
        "            'doc_id': 'Your Document Name',\n",
        "            'sent_id': sent_id,\n",
        "            'eg_id': eg_id,\n",
        "            'index': f'{sent_id}_{eg_id}',\n",
        "            'text': sentence,\n",
        "            'causal_text': causal_text_w_pairs,\n",
        "            'num_rs': num_rs\n",
        "        }\n",
        "        csv_data.append(row)\n",
        "\n",
        "# Write data to CSV file\n",
        "write_to_csv(csv_data, '/content/train_subtask2_grouped.csv')\n",
        "\n",
        "print(\"CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "LoKX1THH3kJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Function to extract causal relations from a sentence\n",
        "def extract_causal_relations(sentence):\n",
        "    # Placeholder function, you would implement this according to your specific causal relation extraction method\n",
        "    # For demonstration purposes, let's assume no causal relations are extracted\n",
        "    return []\n",
        "\n",
        "# Function to write data to CSV file\n",
        "def write_to_csv(data, output_filename):\n",
        "    with open(output_filename, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['corpus', 'doc_id', 'sent_id', 'eg_id', 'index', 'text', 'causal_text', 'num_rs']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Load causal text file\n",
        "with open('/content/domain.txt', 'r') as file:\n",
        "    causal_text = file.read()\n",
        "\n",
        "# Tokenize into sentences\n",
        "sentences = sent_tokenize(causal_text)\n",
        "\n",
        "# Initialize data list to hold rows for CSV\n",
        "csv_data = []\n",
        "\n",
        "# Process each sentence\n",
        "for sent_id, sentence in enumerate(sentences, start=1):\n",
        "    causal_relations = extract_causal_relations(sentence)\n",
        "    eg_id = 0\n",
        "    for index, causal_text_w_pairs in enumerate(causal_relations, start=1):\n",
        "        eg_id += 1\n",
        "        num_rs = len(causal_text_w_pairs)\n",
        "        row = {\n",
        "            'corpus': 'Your Corpus Name',\n",
        "            'doc_id': 'Your Document Name',\n",
        "            'sent_id': sent_id,\n",
        "            'eg_id': eg_id,\n",
        "            'index': f'{sent_id}_{eg_id}',\n",
        "            'text': sentence,\n",
        "            'causal_text': causal_text_w_pairs,\n",
        "            'num_rs': num_rs\n",
        "        }\n",
        "        csv_data.append(row)\n",
        "\n",
        "# Write data to CSV file\n",
        "write_to_csv(csv_data, '/content/train_subtask2_grouped.csv')\n",
        "\n",
        "print(\"CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "3ZGHYglN7cW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Function to extract causal examples and format them\n",
        "def extract_causal_examples(text):\n",
        "    examples = []\n",
        "    domain_start = 0\n",
        "    while True:\n",
        "        domain_start = text.find(\"<ARG0>\", domain_start)\n",
        "        if domain_start == -1:\n",
        "            break\n",
        "\n",
        "        domain_end = text.find(\"</s>\", domain_start)\n",
        "        domain_text = text[domain_start:domain_end]\n",
        "\n",
        "        cause_start = domain_text.find(\"<ARG0>\") + len(\"<ARG0>\")\n",
        "        cause_end = domain_text.find(\"<SIG0>\")\n",
        "        cause = domain_text[cause_start:cause_end].strip()\n",
        "\n",
        "        signal_start = cause_end + len(\"<SIG0>\")\n",
        "        signal_end = domain_text.find(\"</SIG0>\")\n",
        "        signal = domain_text[signal_start:signal_end].strip()\n",
        "\n",
        "        effect_start = signal_end + len(\"</SIG0>\")\n",
        "        effect_end = domain_text.find(\"</ARG1>\")\n",
        "        effect = domain_text[effect_start:effect_end].strip()\n",
        "\n",
        "        examples.append({\n",
        "            'text': cause + ' ' + signal + ' ' + effect,\n",
        "            'causal_text': [cause, signal, effect],\n",
        "            'num_rs': 3\n",
        "        })\n",
        "\n",
        "        domain_start = domain_end + len(\"</s>\")\n",
        "\n",
        "    return examples\n",
        "\n",
        "# Function to write data to CSV file\n",
        "def write_to_csv(data, output_filename):\n",
        "    with open(output_filename, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['corpus', 'doc_id', 'sent_id', 'eg_id', 'index', 'text', 'causal_text', 'num_rs']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/domain.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Extract causal examples\n",
        "examples = extract_causal_examples(text)\n",
        "\n",
        "# Initialize data list to hold rows for CSV\n",
        "csv_data = []\n",
        "\n",
        "# Add examples to data list\n",
        "for i, example in enumerate(examples, start=1):\n",
        "    csv_data.append({\n",
        "        'corpus': 'Your Corpus Name',\n",
        "        'doc_id': f'Document {i}',\n",
        "        'sent_id': 1,\n",
        "        'eg_id': i,\n",
        "        'index': f'{i}_1',\n",
        "        'text': example['text'],\n",
        "        'causal_text': example['causal_text'],\n",
        "        'num_rs': example['num_rs']\n",
        "    })\n",
        "\n",
        "# Write data to CSV file\n",
        "write_to_csv(csv_data, '/content/train_subtask2_grouped.csv')\n",
        "\n",
        "print(\"CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "SGUyAHRUAYVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Load the text file\n",
        "with open('domain.txt', 'r') as file:\n",
        "    examples = file.readlines()\n",
        "\n",
        "# Concatenate the examples into a single string\n",
        "concatenated_examples = ' '.join(examples)\n",
        "\n",
        "# Split the concatenated examples by domain\n",
        "split_examples = concatenated_examples.split('*** Running model.generate:')\n",
        "\n",
        "# Remove the empty first element\n",
        "split_examples = split_examples[1:]\n",
        "\n",
        "# Open a file named 'examples.csv' in write mode\n",
        "with open('examples.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([domains, text_output])\n",
        "\n",
        "    for domain in domains:\n",
        "        # Find the example corresponding to the current domain\n",
        "        for example in split_examples:\n",
        "            if f\"in the domain of {domain}\" in example:\n",
        "                # Extract the generated output for this domain\n",
        "                _, generated_output = example.split('\\n', 1)\n",
        "                generated_output = generated_output.strip()\n",
        "\n",
        "                # Write domain name and generated output to CSV file\n",
        "                writer.writerow([domain, generated_output])\n",
        "                break  # Stop searching for this domain once found"
      ],
      "metadata": {
        "id": "UuhR5KMvaLSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt using the examples you have in your csv. try injecting 2, 3, 5 examples at a time and lets observe the difference by mnaual annotation of the answers\n"
      ],
      "metadata": {
        "id": "FJga49i1DhKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Load examples from the CSV file\n",
        "examples_df = pd.read_csv('/content/train_subtask2_grouped.csv')\n",
        "\n",
        "# Create a function to generate prompts with varying numbers of examples\n",
        "def generate_prompt(examples, num_examples):\n",
        "    # Select the specified number of examples\n",
        "    selected_examples = examples[:num_examples]\n",
        "    prompt = \"\\n\\n\".join(selected_examples)\n",
        "    return prompt\n",
        "\n",
        "# Function to generate and print outputs for different number of examples\n",
        "def generate_outputs(examples_df, num_examples):\n",
        "    prompt = generate_prompt(examples_df['causal_text'].tolist(), num_examples)\n",
        "    print(\"Prompt:\\n\", prompt)\n",
        "    print(\"\\n*** Generating output with\", num_examples, \"examples:\")\n",
        "    output = generator(prompt)\n",
        "    print(\"Generated output:\\n\", output[0]['generated_text'])\n",
        "    print(\"--------------------------------------------------\\n\")\n",
        "\n",
        "# Load the text generation pipeline\n",
        "generator = pipeline(\"text-generation\")\n",
        "\n",
        "# Generate outputs for 2, 3, and 5 examples\n",
        "generate_outputs(examples_df, 2)\n",
        "#generate_outputs(examples_df, 3)\n",
        "#generate_outputs(examples_df, 5)"
      ],
      "metadata": {
        "id": "E2JX8htCAYCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/train_subtask2_grouped.csv')\n",
        "\n",
        "# Function to inject examples and observe manual annotations\n",
        "def inject_and_observe(num_examples):\n",
        "    # Inject examples\n",
        "    injected_examples = df.head(num_examples)\n",
        "\n",
        "    # Print injected examples\n",
        "    print(f\"\\nInjected Examples ({num_examples} examples):\")\n",
        "    for index, row in injected_examples.iterrows():\n",
        "        print(f\"Example {index + 1}:\")\n",
        "        print(f\"Text: {row['text']}\")\n",
        "        print(f\"Causal Text: {row['causal_text']}\")\n",
        "        print()  # Add a newline for better readability\n",
        "\n",
        "    # Here you would manually annotate the injected examples and observe the differences\n",
        "\n",
        "# Inject and observe 2 examples\n",
        "inject_and_observe(2)\n",
        "\n",
        "# Inject and observe 3 examples\n",
        "inject_and_observe(3)\n",
        "\n",
        "# Inject and observe 5 examples\n",
        "inject_and_observe(5)"
      ],
      "metadata": {
        "id": "O0Rv6PAtLfkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqxiDUOVxc7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Concatenation of the previous dataset and the new dataset\n",
        "# Load the previous dataset\n",
        "import pandas as pd\n",
        "previous_df = pd.read_csv('/content/drive/MyDrive/Semester Project/Data/train_subtask2_grouped.csv')\n",
        "print(previous_df.shape)\n",
        "\n",
        "# Load the new dataset\n",
        "new_df = pd.read_csv('/content/train_subtask2_grouped.csv')\n",
        "\n",
        "# Concatenate the previous and new datasets\n",
        "concatenated_df = pd.concat([previous_df, new_df], ignore_index=True)\n",
        "\n",
        "# Save the concatenated dataset to a new CSV file\n",
        "concatenated_df.to_csv('concatenated_dataset.csv', index=False)\n",
        "print(concatenated_df.shape)"
      ],
      "metadata": {
        "id": "HKWRHAYguZfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking manually if there's a causal relation in the text:\n"
      ],
      "metadata": {
        "id": "VoRDEKIt3CFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def manual_check_causal_relation(df):\n",
        "    for index, row in df.iterrows():\n",
        "        print(f\"\\nExample {index + 1}:\")\n",
        "        print(f\"Text: {row['text']}\")\n",
        "        causal_relation = input(\"Does this example contain a causal relation? (Yes/No): \").strip().lower()\n",
        "        while causal_relation not in ['yes', 'no']:\n",
        "            print(\"Invalid input. Please enter 'Yes' or 'No'.\")\n",
        "            causal_relation = input(\"Does this example contain a causal relation? (Yes/No): \").strip().lower()\n",
        "        df.at[index, 'causal_relation'] = causal_relation\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/concatenated_dataset.csv')\n",
        "\n",
        "# Add a column for manual annotations\n",
        "df['causal_relation'] = \"\"\n",
        "\n",
        "# Manually check causal relations\n",
        "df = manual_check_causal_relation(df)\n",
        "\n",
        "# Save the annotated dataset\n",
        "df.to_csv('annotated_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "_RG0w7va3H-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load the annotated dataset\n",
        "df = pd.read_csv('/content/concatenated_dataset.csv')\n",
        "\n",
        "# Check for NaN values in the original DataFrame\n",
        "print(\"NaN values in original DataFrame:\", df.isnull().values.any())\n",
        "\n",
        "# Drop rows with NaN values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Check if the dataset is empty after dropping NaN values\n",
        "if len(df) == 0:\n",
        "    print(\"Dataset is empty after dropping NaN values. Adjust the data or preprocessing steps.\")\n",
        "    exit()\n",
        "\n",
        "# Fill NaN values in the 'text' column with an empty string\n",
        "df['text'].fillna('', inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['causal_text_w_pairs'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Apply the classifier to the entire dataset\n",
        "X_vec = vectorizer.transform(df['text'])\n",
        "df['predicted_causal_relation'] = classifier.predict(X_vec)\n",
        "\n",
        "# Save the updated dataset\n",
        "df.to_csv('predicted_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "gmpRD8FGCcir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}